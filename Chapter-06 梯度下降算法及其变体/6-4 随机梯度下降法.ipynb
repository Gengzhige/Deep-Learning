{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 随机梯度下降法\n",
    "上节我们讲解了梯度下降法，一种深度学习中十分常用而且重要的模型参数最优化方法。明确了它的基本原理后，我们也介绍了它的优缺点。\n",
    "\n",
    "最基本的的梯度下降法是采用全数据集(Full Batch Learning)的形式，它的好处明显。首先，确定的方向能够更好地代表样本总体，能更准确地朝向极值所在的方向。此外，可以更有针对性的选取不同的学习率。但这种方法的缺点很显著。一是不能保证被优化函数达到全局最优解，只有当损失函数为凸函数时,梯度下降算法才能保证达到全局最优解；二是计算时间太长，因为要在全部训练数据上最小化损失，小数据集还行，数据量一多耗时严重；此外，如果函数的形态很复杂，那么梯度下降法可能会在局部最小值附近来回震荡，而不是直接收敛到最优解。梯度下降法对于初始值的选择非常敏感，在训练过程中可能会被卡在局部最优解。因此，在实际应用中，我们通常会使用一些改进的优化算法。本节咱们就来看一种简单的改进版变体：随机梯度下降法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 基本思想\n",
    "\n",
    "随机梯度下降法（SGD，Stochastic Gradient Descent）的基本思想是每次迭代中仅使用一个样本来计算梯度，然后根据梯度来调整参数的值。具体来说，在每次迭代中，我们会随机选择一个样本 $(x_i, y_i)$，并根据当前的参数值 $w$ 计算出损失函数的梯度 $\\nabla L(w; x_i, y_i)$。然后，我们就可以使用梯度下降法的更新公式来调整参数的值：\n",
    "\n",
    "$$ w \\leftarrow w - \\alpha \\cdot \\nabla L(w; x_i, y_i) $$\n",
    "\n",
    "其中 $w$ 表示参数的值，$\\alpha$ 表示学习率，即每次迭代中我们移动的幅度。每次迭代后，我们会检查函数的值是否已经减小到满足预期的程度，如果是则停止迭代，否则继续迭代。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 优缺点比较\n",
    "\n",
    "随机梯度下降法是梯度下降法的改进，因为它在保留了梯度下降法的基本思想的同时，还有一些改进。首先，随机梯度下降法每次迭代中仅使用一个样本计算梯度，这使得计算速度快很多。相比之下，梯度下降法每次迭代中都需要使用所有的样本计算梯度，这使得计算速度要慢很多。其次，随机梯度下降法每次迭代中仅使用一个样本计算梯度，这使得它比梯度下降法更加稳定。\n",
    "\n",
    "事物都是有两面性的。随机梯度下降法也有一些缺点：首先，由于每次迭代中只使用一个样本计算梯度，因此每次迭代的梯度都是有噪声的。这会导致每次迭代的收敛速度变化很大，并且在目标函数有多个局部最小值时可能会被卡在局部最小值中。但是，因为总的迭代次数很多，所以随机梯度下降法最终会收敛到最优解。其次，由于每次迭代中只使用一个样本计算梯度，因此每次迭代的计算代价都很低。但是，如果训练集很大，那么总的计算代价就会变得很高，因为我们需要计算很多次梯度。最后，随机梯度下降法在每次迭代中都只使用一个样本，因此我们不能利用批量计算的优势来加速计算。\n",
    "\n",
    "面对这些缺点，在人工智能的发展史上科学家们开动脑筋想了不少办法，动态学习率就是其中之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.4.3 动态学习率\n",
    "\n",
    "我们知道，学习率是梯度下降算法中非常关键的超参数。当使用常规的梯度下降法（Batch Gradient Descent）训练模型时，我们通常会使用固定的学习率。但是在使用随机梯度下降法时，因为每一次迭代都是随机选择一个样本来计算梯度并更新参数的值，所以使用固定的学习率可能并不是最优的。使用动态学习率可以帮助模型更快地收敛。你可以在每一次迭代之后根据模型的训练情况来动态地调整学习率的值，使得模型能够更快地收敛。\n",
    "\n",
    "在数学表示中，学习率通常记为 $\\alpha$，因此动态学习率可以表示为 $\\alpha_t$，其中 $t$ 表示当前的迭代次数。每一次迭代之后，我们都可以根据模型的训练情况来调整 $\\alpha_t$ 的值。例如，我们可以使用如下策略：\n",
    "\n",
    "$$ \\alpha_t = \\frac{\\alpha_0}{1 + t} $$\n",
    "\n",
    "其中 $\\alpha_0$ 表示初始的学习率。除此之外，还有很多其他的动态学习率策略。这里列举几个常用的动态学习率策略：\n",
    "\n",
    "指数衰减学习率（Exponential Decay Learning Rate）：\n",
    "$$ \\alpha_t = \\alpha_0 \\times \\gamma^t $$\n",
    "\n",
    "其中 $\\alpha_0$ 表示初始的学习率，$\\gamma$ 是一个小于 1 的常数。\n",
    "\n",
    "反比例学习率（Inverse Learning Rate）：\n",
    "$$ \\alpha_t = \\frac{\\alpha_0}{1 + t \\times \\alpha_0} $$\n",
    "\n",
    "其中 $\\alpha_0$ 表示初始的学习率。\n",
    "\n",
    "反比例平方学习率（Inverse Square Learning Rate）：\n",
    "$$ \\alpha_t = \\frac{\\alpha_0}{t^2} $$\n",
    "\n",
    "其中 $\\alpha_0$ 表示初始的学习率。\n",
    "\n",
    "当然，你还可以根据模型的训练情况来自定义动态学习率的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梗直哥提示：尽管动态学习率部分的弥补了随机梯度下降法的缺点，但并没有从根本上解决其固有问题。因此，科学家又进一步提出了一种这种方案，这就是小批量梯度下降法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next 6-5 小批量梯度下降法](./6-5%20小批量梯度下降法.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
